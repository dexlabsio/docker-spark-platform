# Default values for spark-platform.
# This is a YAML-formatted file.

# This is for the secretes for pulling an image from a private repository more information can be found here: https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/
imagePullSecrets: []
# This is to override the chart name.
nameOverride: ""
fullnameOverride: ""

#This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# Add extra labels to all resources
extraLabels: {}

# We only support a standalone master deployment
master:
  image:
    repository: us-east1-docker.pkg.dev/dex-public/spark-platform/spark-all-in-one
    pullPolicy: IfNotPresent
    tag: "v3.5.3"
    command: ["start-master.sh"] 
    args: []

  # This is for setting Kubernetes Annotations to a Pod.
  # For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ 
  podAnnotations: {}
  # This is for setting Kubernetes Labels to a Pod.
  # For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  podLabels: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  environment: {}

  volumes: []
  # - name: foo
  #   secret:
  #     secretName: mysecret
  #     optional: false

  resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

  # Additional volumeMounts on the output Deployment definition.
  volumeMounts: []
  # - name: foo
  #   mountPath: "/etc/foo"
  #   readOnly: true

  nodeSelector: {}

  tolerations: []

  affinity: {}


# Spark worker configuration
worker:
  replicaCount: 1

  image:
    repository: us-east1-docker.pkg.dev/dex-public/spark-platform/spark-all-in-one
    pullPolicy: IfNotPresent
    tag: "v3.5.3"
    command: ["start-worker.sh"] 
    args: []

  # This is for setting Kubernetes Annotations to a Pod.
  # For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ 
  podAnnotations: {}
  # This is for setting Kubernetes Labels to a Pod.
  # For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  podLabels: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  environment: {}

  volumes: []
  # - name: foo
  #   secret:
  #     secretName: mysecret
  #     optional: false

  resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

  # Additional volumeMounts on the output Deployment definition.
  volumeMounts: []
  # - name: foo
  #   mountPath: "/etc/foo"
  #   readOnly: true

  nodeSelector: {}

  tolerations: []

  affinity: {}

  #This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80


# Thrift server configuration
thriftserver:
  replicaCount: 1

  image:
    repository: us-east1-docker.pkg.dev/dex-public/spark-platform/spark-all-in-one
    pullPolicy: IfNotPresent
    tag: "v3.5.3"
    command: ["start-thriftserver.sh"] 
    args: []

  # This is for setting Kubernetes Annotations to a Pod.
  # For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations/ 
  podAnnotations: {}
  # This is for setting Kubernetes Labels to a Pod.
  # For more information checkout: https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
  podLabels: {}

  podSecurityContext: {}
    # fsGroup: 2000

  securityContext: {}
    # capabilities:
    #   drop:
    #   - ALL
    # readOnlyRootFilesystem: true
    # runAsNonRoot: true
    # runAsUser: 1000

  environment: {}

  volumes: []
  # - name: foo
  #   secret:
  #     secretName: mysecret
  #     optional: false

  resources: {}
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

  # Additional volumeMounts on the output Deployment definition.
  volumeMounts: []
  # - name: foo
  #   mountPath: "/etc/foo"
  #   readOnly: true

  nodeSelector: {}

  tolerations: []

  affinity: {}

  #This section is for setting up autoscaling more information can be found here: https://kubernetes.io/docs/concepts/workloads/autoscaling/
  autoscaling:
    enabled: false
    minReplicas: 1
    maxReplicas: 100
    targetCPUUtilizationPercentage: 80
    # targetMemoryUtilizationPercentage: 80


# Shared config as environment variables between master and worker nodes
config: {}
# SPARK_PUBLIC_DNS: example.com
# SPARK_MASTER_PORT: 7077
# SPARK_MASTER_WEBUI_PORT: 8080
# SPARK_WORKER_MEMORY: 10g
# SPARK_WORKER_CORES: 6



# This block is for setting up the ingress for more information can be found here: https://kubernetes.io/docs/concepts/services-networking/ingress/
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

# Override spark defaults
spark:
  # master: "spark://spark-cluster:7077"

  eventLog:
    enabled: "true"
    dir: "/tmp/spark-events"

  serializer: "org.apache.spark.serializer.KryoSerializer"

  executor:
    memory: "2g"
    cores: "1"
    port:
      minRange: "7101"
      maxRange: "7200"

  driver:
    memory: "1g"
    extraClassPath: "/opt/spark/jars/*"
    port:
      minRange: "7001"
      maxRange: "7100"

  dynamicAllocation:
    enabled: "true"
    minExecutors: "1"
    maxExecutors: "2"
    initialExecutors: "1"
    shuffleTracking:
      enabled: "true"

  shuffle:
    service:
      enabled: "true"
      port:
        minRange: "7301"
        maxRange: "7400"

  history:
    fs:
      logDirectory: "/tmp/spark-events"

  sql:
    catalogImplementation: "hive"
    warehouse:
      dir: "s3a://warehouse/"
    hive:
      metastore:
        version: "2.3.9"
        jars: "maven"
        sharedPrefixes: "com.postgresql,org.postgresql"
        # uris: "thrift://spark-cluster:10000"

  hadoop:
    mapreduce:
      fileoutputcommitter:
        algorithm:
          version: "2"
    javax:
      jdo:
        option:
          ConnectionURL: "jdbc:postgresql://hive-metastore:9083/metastore"
          ConnectionDriverName: "org.postgresql.Driver"
          ConnectionUserName: "spark"
          ConnectionPassword: "spark"
          Mapping: "postgres"
    hive:
      metastore:
        schema:
          verification:
            enabled: "true"
            record:
              version: "false"
      server2:
        thrift:
          port: "10000"
    datanucleus:
      autoCreateSchema: "true"
      fixedDatastore: "false"
      schema:
        autoCreateAll: "true"
        validateTables: "false"
        validateConstraints: "false"
    fs:
      s3a:
        endpoint: "http://minio:9000"
        accessKey: "minioadmin"
        secretKey: "minioadmin"
        pathStyleAccess: "true"
        impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"

  port:
    maxRetries: "100"

  ui:
    port:
      maxRetries: "100"
    minPort: "4040"
    maxPort: "4060"

  blockManager:
    port:
      minRange: "10001"
      maxRange: "10100"

  master:
    rest:
      port:
        minRange: "6066"
        maxRange: "6076"

  worker:
    port:
      minRange: "8081"
      maxRange: "8081"
